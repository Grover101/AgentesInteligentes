{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46dbbc31-9ff8-4fe0-ae40-8394185d151b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def relu(x):\n",
    "  return np.maximum(0, x)\n",
    "\n",
    "def reluPrime(x):\n",
    "  return x > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9cc1ea0-7a1a-4b4d-a871-cbbff86c9171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.exp(x).sum(axis=-1,keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b443d47-20f3-49ff-8864-a6274c7917e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Square Error -> usada para regresión (con activación lineal)\n",
    "def mse(y, y_hat):\n",
    "#     print(y.shape, y_hat.shape)\n",
    "    return np.mean((y_hat - y.reshape(y_hat.shape))**2)\n",
    "\n",
    "# Binary Cross Entropy -> usada para clasificación binaria (con sigmoid)\n",
    "def bce(y, y_hat):\n",
    "    return - np.mean(y.reshape(y_hat.shape)*np.log(y_hat) - (1 - y.reshape(y_hat.shape))*np.log(1 - y_hat))\n",
    "\n",
    "# Cross Entropy (aplica softmax + cross entropy de manera estable) -> usada para clasificación multiclase\n",
    "def crossentropy(y, y_hat):\n",
    "#     print(y)\n",
    "#     print(len(y_hat))\n",
    "#     print(np.arange(len(y_hat)))\n",
    "    logits = y_hat[np.arange(len(y_hat)),y]\n",
    "    entropy = - logits + np.log(np.sum(np.exp(y_hat),axis=-1))\n",
    "    return entropy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4aae91e4-1ffb-4a82-ab99-ebb5afdd95fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_mse(y, y_hat):\n",
    "    return y_hat - y.reshape(y_hat.shape)\n",
    "\n",
    "def grad_bce(y, y_hat):\n",
    "    return y_hat - y.reshape(y_hat.shape)\n",
    "\n",
    "def grad_crossentropy(y, y_hat):\n",
    "    answers = np.zeros_like(y_hat)\n",
    "    answers[np.arange(len(y_hat)),y] = 1    \n",
    "    return (- answers + softmax(y_hat)) / y_hat.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11c8531f-5d0f-450b-9a8f-30d5a8c601fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clase base MLP \n",
    "\n",
    "class MLP():\n",
    "  def __init__(self, D_in, H, D_out, loss, grad_loss, activation):\n",
    "    # pesos de la capa 1\n",
    "    self.w1, self.b1 = np.random.normal(loc=0.0,\n",
    "                                  scale=np.sqrt(2/(D_in+H)),\n",
    "                                  size=(D_in, H)), np.zeros(H)\n",
    "    # pesos de la capa 2\n",
    "    self.w2, self.b2 = np.random.normal(loc=0.0,\n",
    "                                  scale=np.sqrt(2/(H+D_out)),\n",
    "                                  size=(H, D_out)), np.zeros(D_out)\n",
    "    self.ws = []\n",
    "    # función de pérdida y derivada\n",
    "    self.loss = loss\n",
    "    self.grad_loss = grad_loss\n",
    "    # función de activación\n",
    "    self.activation = activation\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # salida de la capa 1\n",
    "    self.h_pre = np.dot(x, self.w1) + self.b1\n",
    "    self.h = relu(self.h_pre)\n",
    "    # salida del MLP\n",
    "    y_hat = np.dot(self.h, self.w2) + self.b2 \n",
    "    return self.activation(y_hat)\n",
    "    \n",
    "  def fit(self, X, Y, epochs = 100, lr = 0.001, batch_size=None, verbose=True, log_each=1):\n",
    "    batch_size = len(X) if batch_size == None else batch_size\n",
    "    batches = len(X) // batch_size\n",
    "#     print(batches)\n",
    "    l = []\n",
    "    for e in range(1,epochs+1):     \n",
    "        # Mini-Batch Gradient Descent\n",
    "        _l = []\n",
    "        for b in range(batches):\n",
    "            # batch de datos\n",
    "            x = X[b*batch_size:(b+1)*batch_size]\n",
    "            y = Y[b*batch_size:(b+1)*batch_size]\n",
    "#             print(x)\n",
    "            # salida del perceptrón\n",
    "            y_pred = self(x)\n",
    "#             print(y_pred.shape, y.shape)\n",
    "            # función de pérdida\n",
    "            loss = self.loss(y, y_pred)\n",
    "            _l.append(loss)        \n",
    "            # Backprop \n",
    "            dldy = self.grad_loss(y, y_pred) \n",
    "            grad_w2 = np.dot(self.h.T, dldy)\n",
    "            grad_b2 = dldy.mean(axis=0)\n",
    "            dldh = np.dot(dldy, self.w2.T)*reluPrime(self.h_pre)      \n",
    "            grad_w1 = np.dot(x.T, dldh)\n",
    "            grad_b1 = dldh.mean(axis=0)\n",
    "            # Update (GD)\n",
    "            self.w1 = self.w1 - lr * grad_w1\n",
    "            self.b1 = self.b1 - lr * grad_b1\n",
    "            self.w2 = self.w2 - lr * grad_w2\n",
    "            self.b2 = self.b2 - lr * grad_b2\n",
    "        l.append(np.mean(_l))\n",
    "        # guardamos pesos intermedios para visualización\n",
    "        self.ws.append((\n",
    "            self.w1.copy(),\n",
    "            self.b1.copy(),\n",
    "            self.w2.copy(),\n",
    "            self.b2.copy()\n",
    "        ))\n",
    "        if verbose and not e % log_each:\n",
    "            print(f'Epoch: {e}/{epochs}, Loss: {np.mean(l):.5f}')\n",
    "\n",
    "  def predict(self, ws, x):\n",
    "    w1, b1, w2, b2 = ws\n",
    "    h = relu(np.dot(x, w1) + b1)\n",
    "    y_hat = np.dot(h, w2) + b2\n",
    "    return self.activation(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "000068b3-6956-401a-9062-5e7ed6c29f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP para regresión\n",
    "class MLPRegression(MLP):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super().__init__(D_in, H, D_out, mse, grad_mse, linear)\n",
    "\n",
    "# MLP para clasificación binaria\n",
    "class MLPBinaryClassification(MLP):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super().__init__(D_in, H, D_out, bce, grad_bce, sigmoid)\n",
    "\n",
    "# MLP para clasificación multiclase\n",
    "class MLPClassification(MLP):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super().__init__(D_in, H, D_out, crossentropy, grad_crossentropy, linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "340bcf0a-c898-44c0-b42a-6eeadadb40f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1439 159\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = np.genfromtxt('winequality-red.csv',delimiter=',')\n",
    "\n",
    "trainPorcen = int(len(data)/100*90)\n",
    "testPorcen = int(len(data)/100*10)\n",
    "print(trainPorcen, testPorcen)\n",
    "\n",
    "X_test = data[trainPorcen:, :11]\n",
    "y_test = data[trainPorcen:, 11]\n",
    "\n",
    "X_train = data[:trainPorcen, :11]\n",
    "y_train = data[:trainPorcen, 11]\n",
    "\n",
    "# print(X_test,X_train)\n",
    "\n",
    "# X = data[:, :11]\n",
    "# y = data[:, 11]\n",
    "y_train = np.array([int(e) for e in y_train])\n",
    "y_train = np.squeeze(y_train)\n",
    "\n",
    "\n",
    "y_test = np.array([int(e) for e in y_test])\n",
    "y_test = np.squeeze(y_test)\n",
    "\n",
    "# print(y_train)\n",
    "\n",
    "# print(X)\n",
    "# print(y_trian)\n",
    "# print(X[y==3, 4])\n",
    "X_train.shape, y_train.shape\n",
    "X_mean, X_std = X_train.mean(axis=0), X_train.std(axis=0)\n",
    "X_norm = (X_train - X_mean) / X_std\n",
    "# print( X_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91c56e3e-f1b1-4c89-a2c1-7ecae1d6e726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/10, Loss: 2.27858\n"
     ]
    }
   ],
   "source": [
    "model = MLPClassification(D_in=11, H=10, D_out=11)\n",
    "epochs, lr = 10, 0.2\n",
    "model.fit(X_norm, y_train, epochs, lr, log_each=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "917c5691-b95a-447a-8edf-69762782f374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-27.20080125 -10.2916819  -22.44421062 ...  -5.11936246 -11.28769504\n",
      "   -2.41987071]\n",
      " [-12.25452676  -0.64423429  -6.73750781 ...  -4.2027472   -4.66031318\n",
      "   -2.08059335]\n",
      " [-28.25358402 -12.43344126 -24.2666966  ...  -2.52708203 -10.30317958\n",
      "   -1.58224745]\n",
      " ...\n",
      " [-16.309514    -0.91180104  -8.12327109 ...  -7.56939623  -6.20096405\n",
      "   -1.49588234]\n",
      " [-17.28929656  -1.24742436  -8.63892352 ...  -8.31611239  -6.51812314\n",
      "   -1.28785267]\n",
      " [-15.18912627  -3.12092466 -10.34233677 ...  -4.59394473  -6.06513688\n",
      "   -2.49019291]]\n"
     ]
    }
   ],
   "source": [
    "last = model.ws.pop()\n",
    "# print(last)\n",
    "all_theta = model.predict(last,X_test)\n",
    "print(all_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3d73ca8-e7f3-4a62-ad5c-a875429170d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 6 6 6 6 6 6 6 6 5 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 5 5 6 6 6 6 6 6 6 6 5 6\n",
      " 5 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 5 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 5 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 5 6 5 6 5 6] [6 7 6 5 5 6 6 5 5 5 8 7 7 7 5 6 6 6 5 5 7 6 4 6 6 5 5 7 4 7 3 5 5 6 5 5 7\n",
      " 5 7 3 5 4 5 4 5 4 5 5 5 5 6 6 5 5 5 7 6 5 6 6 6 5 5 5 6 6 3 6 6 6 5 6 5 6\n",
      " 6 6 6 5 6 5 5 6 4 5 5 6 5 6 6 6 6 6 5 6 5 7 6 6 6 5 5 6 7 6 6 7 6 5 5 5 8\n",
      " 5 5 6 5 6 7 5 6 5 5 5 5 5 5 5 6 6 5 5 6 6 6 5 6 6 6 6 6 6 5 6 5 5 5 7 6 6\n",
      " 6 6 5 6 6 6 6 5 6 6 5 6]\n"
     ]
    }
   ],
   "source": [
    "p = np.argmax(all_theta, axis = 1)\n",
    "print(p,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "705c3894-97b8-4a88-bcbb-720b0466cdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 45.625000\n"
     ]
    }
   ],
   "source": [
    "print('Test Set Accuracy: %f' % (np.mean(p == y_test[:]) * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
